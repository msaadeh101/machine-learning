{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Feature Engineering Pipeline\n",
    "\n",
    "This notebook demonstrates production-ready feature engineering practices for MLOps workflows.\n",
    "It includes data validation, feature creation, transformation pipelines, and monitoring components.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Imports](#setup)\n",
    "2. [Data Loading and Validation](#data-loading)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Engineering Pipeline](#feature-engineering)\n",
    "5. [Feature Validation and Quality Checks](#validation)\n",
    "6. [Pipeline Serialization and Deployment](#deployment)\n",
    "7. [Monitoring and Logging](#monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import all necessary libraries for production feature engineering including:\n",
    "- Data manipulation and analysis\n",
    "- Feature engineering and preprocessing\n",
    "- Pipeline creation and serialization\n",
    "- Logging and monitoring\n",
    "- Data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "# Scikit-learn for preprocessing and pipeline creation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler,\n",
    "    LabelEncoder, OneHotEncoder, OrdinalEncoder,\n",
    "    PolynomialFeatures, PowerTransformer\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Serialization and deployment\n",
    "import joblib\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Logging/monitoring\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "# Data Validation\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Config\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 77\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Setup completed at: {datetime.now()}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Validation\n",
    "\n",
    "Load data with comprehensive validation including:\n",
    "- Schema validation\n",
    "- Data quality checks\n",
    "- Missing value anlaysis\n",
    "- Data type validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to log file and console\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('feature_engineering.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "# Logger object named after current module\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Data validation class for production feature engineering.\n",
    "    Performs comprehensive data quality checks and schema validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, expected_schema: Dict[str, str]):\n",
    "        self.expected_schema = expected_schema\n",
    "        self.validation_results = {}\n",
    "\n",
    "    def validate_schema(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Validate dataframe schema against expected schema.\"\"\"\n",
    "        logger.info(\"Starting schema validation\")\n",
    "\n",
    "        missing_cols = set(self.expected_schema.keys()) - set(df.columns)\n",
    "        extra_cols = set(df.columns) - set(self.expected_schema.keys())\n",
    "\n",
    "        if missing_cols:\n",
    "            logger.error(f\"Missing columns: {missing_cols}\")\n",
    "            return False\n",
    "        \n",
    "        if extra_cols:\n",
    "            logger.warning(f\"Unexpected columns found: {extra_cols}\")\n",
    "\n",
    "        # Validate data types, iterate over each real col and expected data type\n",
    "        type_mismatches = []\n",
    "        for col, expected_type in self.expected_schema.items():\n",
    "            if col in df.columns:\n",
    "                actual_type = str(df[col].dtype)\n",
    "                # lenient, as int might appear in int64\n",
    "                if expected_type not in actual_type and actual_type not in expected_type:\n",
    "                    type_mismatches.append((col, expected_type, actual_type))\n",
    "\n",
    "        if type_mismatches:\n",
    "            logger.warning(f\"Data type mismatches: {type_mismatches}\")\n",
    "\n",
    "        logger.info(\"Schema validation completed!\")\n",
    "        return True\n",
    "    \n",
    "    def validate_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Perform comprehensive data quality validation.\"\"\"\n",
    "        logger.info(\"Starting data quality validation\")\n",
    "\n",
    "        quality_report = {\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'missing_values': df.isnull().sum().to_dict(),\n",
    "            'missing_percentage': (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "            'duplicate_rows': df.duplicated().sum(),\n",
    "            'data_types': df.dtypes.astype(str).to_dict(),\n",
    "            'memory_usage': df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "        }\n",
    "\n",
    "        # Check for columns with greater than 50% missing values\n",
    "        high_missing_cols = [\n",
    "            col for col, pct in quality_report['missing_precentage'].items()\n",
    "            if pct > 50\n",
    "        ]\n",
    "\n",
    "        if high_missing_cols:\n",
    "            logger.warning(f\"Columns with >50% missing values: {high_missing_cols}\")\n",
    "\n",
    "        # check for columns with 0 variance (np.number is superclass for all numeric types)\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        zero_variance_cols = [col for col in numeric_cols if df[col].var() == 0]\n",
    "\n",
    "        if zero_variance_cols:\n",
    "            logger.warning(f\"Columns with zero variance: {zero_variance_cols}\")\n",
    "        \n",
    "        quality_report['high_missing_columns'] = high_missing_cols\n",
    "        quality_report['zero_variance_columns'] = zero_variance_cols\n",
    "        \n",
    "        logger.info(\"Data quality validation completed!\")\n",
    "        return quality_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset or load one\n",
    "def create_sample_dataset():\n",
    "    \"\"\"Create a sample dataset for demonstration purposes.\"\"\"\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n_samples = 10000\n",
    "    \n",
    "    data = {\n",
    "        'customer_id': range(1, n_samples + 1),\n",
    "        'age': np.random.randint(18, 80, n_samples),\n",
    "        'income': np.random.lognormal(10, 1, n_samples),\n",
    "        'credit_score': np.random.randint(300, 850, n_samples),\n",
    "        'account_balance': np.random.normal(5000, 2000, n_samples),\n",
    "        'num_products': np.random.poisson(2, n_samples),\n",
    "        'tenure_months': np.random.randint(1, 120, n_samples),\n",
    "        'is_active': np.random.choice([0, 1], n_samples, p=[0.3, 0.7]),\n",
    "        'geography': np.random.choice(['Urban', 'Suburban', 'Rural'], n_samples, p=[0.5, 0.3, 0.2]),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples, p=[0.52, 0.48]),\n",
    "        'has_credit_card': np.random.choice([0, 1], n_samples, p=[0.4, 0.6]),\n",
    "        'last_transaction_date': pd.date_range('2023-01-01', '2024-01-01', periods=n_samples),\n",
    "        'churn': None  # Target variable - will be generated based on features\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Generate target variable based on features using a realistic relationship\n",
    "    # We are adding these probabilities together\n",
    "    churn_prob = (\n",
    "        0.1 +                                            # Baseline risk\n",
    "        0.001 * (df['age'] - 40)**2 +                    # U-shaped age effect\n",
    "        -0.00001 * df['income'] +                        # Higher income = lower churn\n",
    "        -0.0002 * df['credit_score'] +                   # Higher credit score = lower churn\n",
    "        -0.01 * df['num_products'] +                     # More products = lower churn\n",
    "        -0.002 * df['tenure_months'] +                   # Longer tenure = lower churn\n",
    "        -0.1 * df['is_active'] +                         # Active users = lower churn\n",
    "        0.05 * (df['geography'] == 'Rural').astype(int)  # Rural = higher churn\n",
    "    )\n",
    "\n",
    "    # Ensure probabilities are between 0 and 1\n",
    "    churn_prob = np.clip(churn_prob, 0, 1)\n",
    "    df['churn'] = np.random.binomial(1, churn_prob, n_samples)\n",
    "    \n",
    "    # Introduce some missing values to simulate real-world data\n",
    "    df.loc[np.random.choice(df.index, size=500, replace=False), 'income'] = np.nan\n",
    "    df.loc[np.random.choice(df.index, size=300, replace=False), 'credit_score'] = np.nan\n",
    "    df.loc[np.random.choice(df.index, size=200, replace=False), 'account_balance'] = np.nan\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and validate data\n",
    "logger.info(\"Loading dataset...\")\n",
    "df = create_sample_dataset()\n",
    "\n",
    "# Define the expected schema\n",
    "expected_schema = {\n",
    "    'customer_id': 'int64',\n",
    "    'age': 'int64',\n",
    "    'income': 'float64',\n",
    "    'credit_score': 'float64',\n",
    "    'account_balance': 'float64',\n",
    "    'num_products': 'int64',\n",
    "    'tenure_months': 'int64',\n",
    "    'is_active': 'int64',\n",
    "    'geography': 'object',\n",
    "    'gender': 'object',\n",
    "    'has_credit_card': 'int64',\n",
    "    'last_transaction_date': 'datetime64[ns]',\n",
    "    'churn': 'int64'\n",
    "}\n",
    "\n",
    "# Initialize validator and perform validation\n",
    "validator = DataValidator(expected_schema)\n",
    "is_valid_schema = validator.validate_schema(df)\n",
    "quality_report = validator.validate_data_quality(df)\n",
    "\n",
    "print(\"Data Quality Report:\")\n",
    "print(json.dumps(quality_report, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Perform comprehensive EDA to understand data patterns and inform feature engineering decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Perform comprehensive exploratory data analysis (EDA).\n",
    "    \"\"\"\n",
    "\n",
    "    logger.info(\"Starting exploratory data analysis\")\n",
    "    \n",
    "    print(\"Dataset Overview:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Describe gives descriptive stats, works only on numeric columns\n",
    "    # display makes the DF look like a table, part of Jupyter\n",
    "    print(\"\\nBasic Stats:\")\n",
    "    display(df.describe(include='all'))\n",
    "\n",
    "    print(\"\\nMissing Values Analysis:\")\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Missing Count': df.isnull().sum(),\n",
    "        'Missing Percentage': (df.isnull().sum() /len(df)) * 100\n",
    "    })\n",
    "    # missing_data['Missing Count'] > 0 Creates a boolean mask: Series of True/False values, one per row\n",
    "    # missing_data[boolean_mask] -> pandas does the filtering, only keeping where condition is true\n",
    "    missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "    # Assign it back to missing_data for clean reassignment\n",
    "    print(missing_data)\n",
    "\n",
    "    # Correlation analysis for numeric features (select only numeric column names)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        # .corr() computes the correlation matrix (pairwise correlations between all numeric columns)\n",
    "        correlation_matrix = df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Target variable distribution\n",
    "    if 'churn' in df.columns:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1) # 1 row, 3 columns of plots, this is the first plot\n",
    "        # plot the distribution of churn values, 0 vs 1\n",
    "        df['churn'].value_counts().plot(kind='bar')\n",
    "        plt.title('Target Variable Distribution')\n",
    "        plt.xlabel('Churn')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Age distribution by churn\n",
    "        plt.subplot(1, 3, 2)\n",
    "        for churn_val in df['churn'].unique():\n",
    "            subset = df[df['churn'] == churn_val]['age'].dropna()\n",
    "            plt.hist(subset, alpha=0.7, label=f'Churn = {churn_val}', bins=20)\n",
    "        plt.title('Age Distribution by Churn')\n",
    "        plt.xlabel('Age')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Income distribution by churn\n",
    "        plt.subplot(1, 3, 3)\n",
    "        for churn_val in df['churn'].unique():\n",
    "            subset = df[df['churn'] == churn_val]['income'].dropna()\n",
    "            plt.hist(subset, alpha=0.7, label=f'Churn = {churn_val}', bins=20)\n",
    "        plt.title('Income Distribution by Churn')\n",
    "        plt.xlabel('Income')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')  # Log scale due to income distribution\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Categorical variable analysis\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        # create a row of subplots, 1 row, N columns (len(categorical_cols))\n",
    "        fig, axes = plt.subplots(1, len(categorical_cols), figsize=(15, 5))\n",
    "        if len(categorical_cols) == 1:\n",
    "            # if only 1 column, then axes will not be a list, so make it one\n",
    "            axes = [axes]\n",
    "            \n",
    "        # loop and plot each into its subplot\n",
    "        for i, col in enumerate(categorical_cols):\n",
    "            df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
    "            axes[i].set_title(f'{col} Distribution')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Perform EDA\n",
    "perform_eda(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering Pipeline\n",
    "\n",
    "Create comprehensive feature engineering pipeline with custom transformers and production-ready components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Sections:\n",
    "\n",
    "1. **Setup and Imports** - All necessary libraries with proper configuration\n",
    "2. **Data Loading and Validation** - Schema validation, data quality checks, and synthetic dataset creation\n",
    "3. **Exploratory Data Analysis** - Comprehensive EDA with visualizations and statistical analysis\n",
    "4. **Feature Engineering Pipeline** - Custom transformers, preprocessing pipelines, and feature creation\n",
    "5. **Feature Validation and Quality Checks** - Distribution shift detection and quality monitoring\n",
    "6. **Pipeline Serialization and Deployment** - Versioning, metadata management, and deployment artifacts\n",
    "7. **Monitoring and Logging** - Production monitoring with drift detection and reporting\n",
    "8. **Model Training and Evaluation** - Performance comparison showing feature engineering benefits\n",
    "9. **Summary and Best Practices** - Key takeaways and production guidelines\n",
    "\n",
    "## Key Features Demonstrated:\n",
    "\n",
    "- **Custom Transformers**: DateFeatureExtractor, BusinessFeatureCreator, OutlierTreatment.\n",
    "- **Production Pipeline**: Complete scikit-learn pipeline with proper preprocessing.\n",
    "- **Validation Framework**: Comprehensive data validation and quality checks.\n",
    "- **Deployment Ready**: Serialization, versioning, and schema management.\n",
    "- **Monitoring System**: Data drift detection and performance tracking.\n",
    "- **Performance Analysis**: Clear demonstration of feature engineering benefits."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
