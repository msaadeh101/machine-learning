{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python core and general purpose libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy \n",
    "### (Numerical Operations, Arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a 1-dimensional NumPy array\n",
    "arr_1d = np.array([1, 2, 3, 4, 5])\n",
    "print(\"NumPy 1D Array:\\n\", arr_1d)\n",
    "# Numpy arrays are homogeneous (all elements of the same type) and allow for\n",
    "# vectorized operations, which are much faster than Python lists for large datasets.\n",
    "\n",
    "# Create a 2-dimensional NumPy array (matrix)\n",
    "arr_2d = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"\\nNumPy 2D Array:\\n\", arr_2d)\n",
    "# Used for representing matrices and higher-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "### (Data analysis, manipulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a Pandas DataFrame from a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'Age': [25, 30, 35, 28],\n",
    "    'City': ['New York', 'London', 'Paris', 'Tokyo']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nPandas DataFrame:\\n\", df)\n",
    "# Explanation: DataFrames are tabular data structures, similar to spreadsheets or SQL tables.\n",
    "# They provide powerful tools for data manipulation, cleaning, analysis, and visualization.\n",
    "\n",
    "# Select a column\n",
    "print(\"\\n'Name' column from DataFrame:\\n\", df['Name'])\n",
    "print(\"\\n'Age' column (values only) for DF:\\n\", df['Age'].values)\n",
    "# Explanation: Easy and intuitive way to access specific columns (Series) of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "### (Plotting, Visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate some data\n",
    "x = np.linspace(0, 10, 100) # 100 evenly spaced points, from 0 - 10\n",
    "y = np.sin(x) # sine wave\n",
    "\n",
    "# Create a simple line plot\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(x, y, label='sin(x)', color='blue', linestyle='--')\n",
    "plt.xlabel(\"X-axis\")\n",
    "plt.xlabel(\"Y-axis\")\n",
    "plt.title(\"Sine Wave Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "### (Machine Learning Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_iris # a sample datset\n",
    "\n",
    "# Load the sample data\n",
    "iris = load_iris()\n",
    "X = iris.data # features, capital X\n",
    "y = iris.target # the labels to predict, lowercase y\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# train_test_split is crucial for evaluating model performance on unseen data, preventing overfitting.\n",
    "\n",
    "# Create a Logistic Regression model\n",
    "model = LogisticRegression(max_iter=200) # increase max_iter to 200 for multi class problems\n",
    "# Instantiating a machine learning model.\n",
    "\n",
    "# Train the Model\n",
    "model.fit(X_train, y_train) # .fit() trains model using provided training data.\n",
    "# Learns patters from X_train to predict y_train\n",
    "\n",
    "# Make Predicitons on test set\n",
    "y_pred = model.predict(X_test) # .predict() uses trained model to make predictions on unseen data\n",
    "\n",
    "# Evaluate model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\Sklearn Logistic Regression accuracy: {accuracy:.2f}\") # Accuracy to 2 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Print predictions vs actuals\n",
    "print(\"\\nPredicted labels:\", y_pred)\n",
    "print(\"Actual labels:   \", y_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index=iris.target_names, columns=iris.target_names)\n",
    "print(\"\\nConfusion Matrix (labeled):\\n\", df_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "### (Deep learning platform, includes Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define simple sequential\n",
    "tf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=10, activation='relu', input_shape=(4,)), # Input layer with 4 features\n",
    "    tf.keras.layers.Dense(units=3, activation='softmax') # Output layer for 3 classes\n",
    "])\n",
    "# keras.Sequential allows building neural networks layer by layer. Dense layers are fully connected.\n",
    "# 'relu' is a common activation function. 'softmax' is for multi-classification outputs (probabilities)\n",
    "\n",
    "# Compile the model configures it for training\n",
    "tf_model.compile(optimizer='adam', # optimizer defines how weights are updated,\n",
    "                loss='sparse_categorical_crossentropy', # loss measures for error\n",
    "                metrics=['accuracy']) # metrics evaluated at training\n",
    "\n",
    "# Provides summary of model architecture, layer types, output shapes, trainable parameters.\n",
    "tf_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tensorflow ouput:\n",
    "- Each Dense Layer is shown above.\n",
    "- Dense(units=X) means X neurons, each connected to all inputs. \n",
    "- Each neuron has:\n",
    "    - one weight per feature\n",
    "    - one bias.\n",
    "- Params = weights = biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch\n",
    "### (Flexible deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # module provides buildign blocks for Neural Networks (NNs)\n",
    "import torch.optim as optim\n",
    "\n",
    "# Createa a simple tensor optimized for deep learning on GPUs\n",
    "pt_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(\"\\nPytorch Tensor:\\n\", pt_tensor)\n",
    "print(\"\\n2D tensor/matrix, shape is 2,2, type is a torch.float32 because of 1.\")\n",
    "\n",
    "# Define a simple NN class for binary classification\n",
    "# Inherits from the nn.Module, base class for all PyTorch NNs\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self): # intialize this class\n",
    "        super(SimpleNN, self).__init__() # super gives your class access to nn.Module parent\n",
    "        self.fc1 = nn.Linear(10, 5) # Fully connected dense layer, Input 10 features, output 5\n",
    "        self.relu = nn.ReLU() # non-linear activation function\n",
    "        self.fc2 = nn.Linear(5, 1) # Another fully connected layer, for binary classif\n",
    "        self.sigmoid = nn.Sigmoid() # sigmoid squashes numbers between 0 and 1, probability\n",
    "\n",
    "    def forward(self, x): # the brain, how model makes predictions\n",
    "        x = self.fc1(x) # Apply first layer (10 -> 5)\n",
    "        x = self.relu(x) # Apply ReLU activation\n",
    "        x = self.fc2(x) # Apply second layer (5 -> 1)\n",
    "        x = self.sigmoid(x) # Return the result into a probability\n",
    "        return x # output prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "pt_model = SimpleNN()\n",
    "print(\"\\nPyTorch Model Architecture:\\n\", pt_model)\n",
    "# Pytorch models are classes inheriting from the nn.Module\n",
    "# __init__ defines the layers and forward defines the data flow through the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "### (Fast, powerful boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import make_classification # Generate synthetic classication\n",
    "\n",
    "# Generate synthetic data and split\n",
    "X_synth, y_synth = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(X_synth, y_synth, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier model\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "# objective defines the loss function for binary classification\n",
    "# use label encoder false - suppress a future deprecation warning\n",
    "\n",
    "# Train the XGBoost model\n",
    "xgb_model.fit(X_train_xgb, y_train_xgb)\n",
    "# .fit() trains the boosting model on training data\n",
    "# XGBoost builds an ensemble of decision trees sequentially\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = xgb_model.predict(X_test_xgb)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test_xgb, y_pred_xgb)\n",
    "print(f\"\\nXGBoost Classifier Accuracy: {accuracy_xgb:.3f}\")\n",
    "# XGBoost is a highly efficient and powerful gradient boosting library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-minimal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
